{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f2952e2-b174-4328-8821-8f385b4d83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import csv\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ec677-c1e0-4fff-a22b-761c2d33209a",
   "metadata": {},
   "source": [
    "## Article IV of the IMF's Articles of Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "199d895f-a27a-4373-a63c-8cd827eaf182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to ChromeDriver\n",
    "s = Service(r'C:\\Users\\Victoria\\OneDrive\\文档\\Dropbox\\Machine Learning Python\\Project\\chromedriver.exe')\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# Base URL setup\n",
    "base_url = 'https://www.imf.org/en/Search#q=Article%20IV&first={}&sort=relevancy&numberOfResults=50&f:type=[PUBS,COUNTRYREPS]'\n",
    "\n",
    "# Range of pages to scrape, adjust 'first' parameter for each URL\n",
    "starts = range(50, 2001, 50)  \n",
    "\n",
    "# Prepare CSV file to store results\n",
    "with open('imf_text.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'URL', 'Date', 'Description'])  # Write header\n",
    "\n",
    "    for start in starts:\n",
    "        # Construct URL for each page by replacing the placeholder with the current 'start' value\n",
    "        url = base_url.format(start)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Allow some time for the page to load\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Scroll down to load all contents on the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Sleep to allow content to load\n",
    "\n",
    "        # Find all articles\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, 'div.CoveoTemplateLoader')\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # Extract title\n",
    "                title_element = article.find_element(By.CSS_SELECTOR, 'a.CoveoResultLink')\n",
    "                title = title_element.text.strip()\n",
    "                url = title_element.get_attribute('href').strip()\n",
    "\n",
    "                # Extract date\n",
    "                date = article.find_element(By.CSS_SELECTOR, 'div.imf-date').text.strip()\n",
    "\n",
    "                # Extract description\n",
    "                description = article.find_element(By.CSS_SELECTOR, 'div.imf-description').text.strip()\n",
    "\n",
    "                # Write the data to the CSV file\n",
    "                writer.writerow([title, url, date, description])\n",
    "\n",
    "            except Exception as e:\n",
    "                # Optionally, you might still want to log errors to a file instead of printing them\n",
    "                pass  # or log to file if needed\n",
    "\n",
    "# Close the driver after scraping\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2298c13e-be67-45ee-bcee-4d8030ce155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to ChromeDriver\n",
    "s = Service(r'C:\\Users\\Victoria\\OneDrive\\文档\\Dropbox\\Machine Learning Python\\Project\\chromedriver.exe')\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# Base URL setup\n",
    "base_url = 'https://www.imf.org/en/Search#q=Article%20IV&first={}&sort=%40imfdate%20descending&numberOfResults=50&f:type=[PUBS,COUNTRYREPS]'\n",
    "\n",
    "# Range of pages to scrape, adjust 'first' parameter for each URL\n",
    "starts = range(50, 1401, 50)  \n",
    "\n",
    "# Prepare CSV file to store results\n",
    "with open('imf_text2.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'URL', 'Date', 'Description'])  # Write header\n",
    "\n",
    "    for start in starts:\n",
    "        # Construct URL for each page by replacing the placeholder with the current 'start' value\n",
    "        url = base_url.format(start)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Allow some time for the page to load\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Scroll down to load all contents on the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Sleep to allow content to load\n",
    "\n",
    "        # Find all articles\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, 'div.CoveoTemplateLoader')\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # Extract title\n",
    "                title_element = article.find_element(By.CSS_SELECTOR, 'a.CoveoResultLink')\n",
    "                title = title_element.text.strip()\n",
    "                url = title_element.get_attribute('href').strip()\n",
    "\n",
    "                # Extract date\n",
    "                date = article.find_element(By.CSS_SELECTOR, 'div.imf-date').text.strip()\n",
    "\n",
    "                # Extract description\n",
    "                description = article.find_element(By.CSS_SELECTOR, 'div.imf-description').text.strip()\n",
    "\n",
    "                # Write the data to the CSV file\n",
    "                writer.writerow([title, url, date, description])\n",
    "\n",
    "            except Exception as e:\n",
    "                # Optionally, you might still want to log errors to a file instead of printing them\n",
    "                pass  # or log to file if needed\n",
    "\n",
    "# Close the driver after scraping\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b41255c-3bc7-4585-ab0a-c0d4eb31539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to ChromeDriver\n",
    "s = Service(r'C:\\Users\\Victoria\\OneDrive\\文档\\Dropbox\\Machine Learning Python\\Project\\chromedriver.exe')\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# Base URL setup\n",
    "base_url = 'https://www.imf.org/en/Search#q=Article%20IV&first={}&sort=%40imfdate%20ascending&numberOfResults=50&f:type=[PUBS,COUNTRYREPS,ARTICLE4]'\n",
    "\n",
    "# Range of pages to scrape, adjust 'first' parameter for each URL\n",
    "starts = range(50, 1401, 50)  \n",
    "\n",
    "# Prepare CSV file to store results\n",
    "with open('imf_text3.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'URL', 'Date', 'Description'])  # Write header\n",
    "\n",
    "    for start in starts:\n",
    "        # Construct URL for each page by replacing the placeholder with the current 'start' value\n",
    "        url = base_url.format(start)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Allow some time for the page to load\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Scroll down to load all contents on the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Sleep to allow content to load\n",
    "\n",
    "        # Find all articles\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, 'div.CoveoTemplateLoader')\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # Extract title\n",
    "                title_element = article.find_element(By.CSS_SELECTOR, 'a.CoveoResultLink')\n",
    "                title = title_element.text.strip()\n",
    "                url = title_element.get_attribute('href').strip()\n",
    "\n",
    "                # Extract date\n",
    "                date = article.find_element(By.CSS_SELECTOR, 'div.imf-date').text.strip()\n",
    "\n",
    "                # Extract description\n",
    "                description = article.find_element(By.CSS_SELECTOR, 'div.imf-description').text.strip()\n",
    "\n",
    "                # Write the data to the CSV file\n",
    "                writer.writerow([title, url, date, description])\n",
    "\n",
    "            except Exception as e:\n",
    "                # Optionally, you might still want to log errors to a file instead of printing them\n",
    "                pass  # or log to file if needed\n",
    "\n",
    "# Close the driver after scraping\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac6764f-51b7-4b0d-8ea0-dba94221f0f9",
   "metadata": {},
   "source": [
    "## IMF Financial Sector Assessment Program (FSAP) Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "386b742d-58ef-4140-a9da-b516df3a821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to ChromeDriver\n",
    "s = Service(r'C:\\Users\\Victoria\\OneDrive\\文档\\Dropbox\\Machine Learning Python\\Project\\chromedriver.exe')\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# Base URL setup\n",
    "base_url = 'https://www.imf.org/en/Search#q=Article%20IV&first={}&sort=%40imfdate%20descending&numberOfResults=50&f:type=[PUBS,COUNTRYREPS,FSAPS]'\n",
    "\n",
    "# Range of pages to scrape, adjust 'first' parameter for each URL\n",
    "starts = range(50, 451, 50)  \n",
    "\n",
    "# Prepare CSV file to store results\n",
    "with open('imf_text4.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'URL', 'Date', 'Description'])  # Write header\n",
    "\n",
    "    for start in starts:\n",
    "        # Construct URL for each page by replacing the placeholder with the current 'start' value\n",
    "        url = base_url.format(start)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Allow some time for the page to load\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Scroll down to load all contents on the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Sleep to allow content to load\n",
    "\n",
    "        # Find all articles\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, 'div.CoveoTemplateLoader')\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # Extract title\n",
    "                title_element = article.find_element(By.CSS_SELECTOR, 'a.CoveoResultLink')\n",
    "                title = title_element.text.strip()\n",
    "                url = title_element.get_attribute('href').strip()\n",
    "\n",
    "                # Extract date\n",
    "                date = article.find_element(By.CSS_SELECTOR, 'div.imf-date').text.strip()\n",
    "\n",
    "                # Extract description\n",
    "                description = article.find_element(By.CSS_SELECTOR, 'div.imf-description').text.strip()\n",
    "\n",
    "                # Write the data to the CSV file\n",
    "                writer.writerow([title, url, date, description])\n",
    "\n",
    "            except Exception as e:\n",
    "                # Optionally, you might still want to log errors to a file instead of printing them\n",
    "                pass  # or log to file if needed\n",
    "\n",
    "# Close the driver after scraping\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e054c-33df-427e-95e7-8788359a3d09",
   "metadata": {},
   "source": [
    "## IMF Debt Sustainability Analysis (DSA) Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc119a13-5a09-4f72-86fb-207175847f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to ChromeDriver\n",
    "s = Service(r'C:\\Users\\Victoria\\OneDrive\\文档\\Dropbox\\Machine Learning Python\\Project\\chromedriver.exe')\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# Base URL setup\n",
    "base_url = 'https://www.imf.org/en/Search#q=Article%20IV&first={}&sort=%40imfdate%20descending&numberOfResults=50&f:type=[PUBS,COUNTRYREPS,DSAS]'\n",
    "\n",
    "# Range of pages to scrape, adjust 'first' parameter for each URL\n",
    "starts = range(50, 401, 50)  \n",
    "\n",
    "# Prepare CSV file to store results\n",
    "with open('imf_text5.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'URL', 'Date', 'Description'])  # Write header\n",
    "\n",
    "    for start in starts:\n",
    "        # Construct URL for each page by replacing the placeholder with the current 'start' value\n",
    "        url = base_url.format(start)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Allow some time for the page to load\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Scroll down to load all contents on the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Sleep to allow content to load\n",
    "\n",
    "        # Find all articles\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, 'div.CoveoTemplateLoader')\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # Extract title\n",
    "                title_element = article.find_element(By.CSS_SELECTOR, 'a.CoveoResultLink')\n",
    "                title = title_element.text.strip()\n",
    "                url = title_element.get_attribute('href').strip()\n",
    "\n",
    "                # Extract date\n",
    "                date = article.find_element(By.CSS_SELECTOR, 'div.imf-date').text.strip()\n",
    "\n",
    "                # Extract description\n",
    "                description = article.find_element(By.CSS_SELECTOR, 'div.imf-description').text.strip()\n",
    "\n",
    "                # Write the data to the CSV file\n",
    "                writer.writerow([title, url, date, description])\n",
    "\n",
    "            except Exception as e:\n",
    "                # Optionally, you might still want to log errors to a file instead of printing them\n",
    "                pass  # or log to file if needed\n",
    "\n",
    "# Close the driver after scraping\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4517d802-ba00-47e8-906c-5d44020a2d2f",
   "metadata": {},
   "source": [
    "## IMF World Economic Outlook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9996b87-0596-499c-90d3-c875d0f90d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting data from article: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"p:nth-of-type(2) > span\"}\n",
      "  (Session info: chrome=123.0.6312.123); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF722377032+63090]\n",
      "\t(No symbol) [0x00007FF7222E2C82]\n",
      "\t(No symbol) [0x00007FF72217EC65]\n",
      "\t(No symbol) [0x00007FF7221C499D]\n",
      "\t(No symbol) [0x00007FF7221C4ADC]\n",
      "\t(No symbol) [0x00007FF7221BA0AC]\n",
      "\t(No symbol) [0x00007FF7221E701F]\n",
      "\t(No symbol) [0x00007FF7221BA00A]\n",
      "\t(No symbol) [0x00007FF7221E71F0]\n",
      "\t(No symbol) [0x00007FF722203412]\n",
      "\t(No symbol) [0x00007FF7221E6D83]\n",
      "\t(No symbol) [0x00007FF7221B83A8]\n",
      "\t(No symbol) [0x00007FF7221B9441]\n",
      "\tGetHandleVerifier [0x00007FF7227725AD+4238317]\n",
      "\tGetHandleVerifier [0x00007FF7227AF70D+4488525]\n",
      "\tGetHandleVerifier [0x00007FF7227A79EF+4456495]\n",
      "\tGetHandleVerifier [0x00007FF722450576+953270]\n",
      "\t(No symbol) [0x00007FF7222EE54F]\n",
      "\t(No symbol) [0x00007FF7222E9224]\n",
      "\t(No symbol) [0x00007FF7222E935B]\n",
      "\t(No symbol) [0x00007FF7222D9B94]\n",
      "\tBaseThreadInitThunk [0x00007FFB8AF17344+20]\n",
      "\tRtlUserThreadStart [0x00007FFB8CF226B1+33]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Specify the path to ChromeDriver\n",
    "s = Service(r'C:\\Users\\Victoria\\OneDrive\\文档\\Dropbox\\Machine Learning Python\\Project\\chromedriver.exe')\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# Base URL setup\n",
    "base_url = 'https://www.imf.org/en/publications/weo?page={}'\n",
    "\n",
    "# Range of pages to scrape\n",
    "starts = range(1, 9)  # Adjust the range as needed\n",
    "\n",
    "# Prepare CSV file to store results\n",
    "with open('imf_text6.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'URL', 'Date', 'Description'])  # Write header\n",
    "\n",
    "    for start in starts:\n",
    "        # Construct URL for each page by replacing the placeholder with the current 'start' value\n",
    "        url = base_url.format(start)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Allow some time for the page to load\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Scroll down to load all contents on the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Sleep to allow content to load\n",
    "\n",
    "        # Find all articles\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, 'div.result-row.pub-row')\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # Extract title and URL\n",
    "                title_element = article.find_element(By.CSS_SELECTOR, 'h6 > a')\n",
    "                title = title_element.text.strip()\n",
    "                url = title_element.get_attribute('href').strip()\n",
    "\n",
    "                # Extract date\n",
    "                date = article.find_element(By.CSS_SELECTOR, 'p').text.strip()\n",
    "\n",
    "                # Extract description\n",
    "                description_element = article.find_element(By.CSS_SELECTOR, 'p:nth-of-type(2) > span')\n",
    "                description = description_element.text.strip()\n",
    "\n",
    "                # Write the data to the CSV file\n",
    "                writer.writerow([title, url, date, description])\n",
    "\n",
    "            except Exception as e:\n",
    "                pass \n",
    "\n",
    "# Close the driver after scraping\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22275b18-b6c2-43b1-acaa-d627ac49880b",
   "metadata": {},
   "source": [
    "## Global Financial Stability Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913ccede-26ff-42bc-9134-bd570e8ac8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the path to ChromeDriver\n",
    "s = Service(r'C:\\Users\\Victoria\\OneDrive\\文档\\Dropbox\\Machine Learning Python\\Project\\chromedriver.exe')\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# Base URL setup\n",
    "base_url = 'https://www.imf.org/en/Publications/GFSR?page{}'\n",
    "\n",
    "# Range of pages to scrape\n",
    "starts = range(1, 5)  # Adjust the range as needed\n",
    "\n",
    "# Prepare CSV file to store results\n",
    "with open('imf_text7.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'URL', 'Date', 'Description'])  # Write header\n",
    "\n",
    "    for start in starts:\n",
    "        # Construct URL for each page by replacing the placeholder with the current 'start' value\n",
    "        url = base_url.format(start)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Allow some time for the page to load\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Scroll down to load all contents on the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Sleep to allow content to load\n",
    "\n",
    "        # Find all articles\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, 'div.result-row.pub-row')\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # Extract title and URL\n",
    "                title_element = article.find_element(By.CSS_SELECTOR, 'h6 > a')\n",
    "                title = title_element.text.strip()\n",
    "                url = title_element.get_attribute('href').strip()\n",
    "\n",
    "                # Extract date\n",
    "                date = article.find_element(By.CSS_SELECTOR, 'p').text.strip()\n",
    "\n",
    "                # Extract description\n",
    "                description_element = article.find_element(By.CSS_SELECTOR, 'p:nth-of-type(2) > span')\n",
    "                description = description_element.text.strip()\n",
    "\n",
    "                # Write the data to the CSV file\n",
    "                writer.writerow([title, url, date, description])\n",
    "\n",
    "            except Exception as e:\n",
    "                pass \n",
    "\n",
    "# Close the driver after scraping\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b021976-da01-4b54-a079-34aa2c182e44",
   "metadata": {},
   "source": [
    "## Fiscal Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "071001cd-df71-4965-8b50-33af23fd8d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the path to ChromeDriver\n",
    "s = Service(r'C:\\Users\\Victoria\\OneDrive\\文档\\Dropbox\\Machine Learning Python\\Project\\chromedriver.exe')\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# Base URL setup\n",
    "base_url = 'https://www.imf.org/en/Publications/FM?page{}'\n",
    "\n",
    "# Range of pages to scrape\n",
    "starts = range(1, 4)  # Adjust the range as needed\n",
    "\n",
    "# Prepare CSV file to store results\n",
    "with open('imf_text8.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'URL', 'Date', 'Description'])  # Write header\n",
    "\n",
    "    for start in starts:\n",
    "        # Construct URL for each page by replacing the placeholder with the current 'start' value\n",
    "        url = base_url.format(start)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Allow some time for the page to load\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Scroll down to load all contents on the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Sleep to allow content to load\n",
    "\n",
    "        # Find all articles\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, 'div.result-row.pub-row')\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # Extract title and URL\n",
    "                title_element = article.find_element(By.CSS_SELECTOR, 'h6 > a')\n",
    "                title = title_element.text.strip()\n",
    "                url = title_element.get_attribute('href').strip()\n",
    "\n",
    "                # Extract date\n",
    "                date = article.find_element(By.CSS_SELECTOR, 'p').text.strip()\n",
    "\n",
    "                # Extract description\n",
    "                description_element = article.find_element(By.CSS_SELECTOR, 'p:nth-of-type(2) > span')\n",
    "                description = description_element.text.strip()\n",
    "\n",
    "                # Write the data to the CSV file\n",
    "                writer.writerow([title, url, date, description])\n",
    "\n",
    "            except Exception as e:\n",
    "                pass \n",
    "\n",
    "# Close the driver after scraping\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f151c3-3966-41c1-93f3-af7219d03d0e",
   "metadata": {},
   "source": [
    "## Regional Economic Outlook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e1ccac3-3220-4f62-8bf3-bb0fd7018eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the path to ChromeDriver\n",
    "s = Service(r'C:\\Users\\Victoria\\OneDrive\\文档\\Dropbox\\Machine Learning Python\\Project\\chromedriver.exe')\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# Base URL setup\n",
    "base_url = 'https://www.imf.org/en/Publications/REO?page={}'\n",
    "\n",
    "# Range of pages to scrape\n",
    "starts = range(1, 18,1)  # Adjust the range as needed\n",
    "\n",
    "# Prepare CSV file to store results\n",
    "with open('imf_text9.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'URL', 'Date', 'Description'])  # Write header\n",
    "\n",
    "    for start in starts:\n",
    "        # Construct URL for each page by replacing the placeholder with the current 'start' value\n",
    "        url = base_url.format(start)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Allow some time for the page to load\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Scroll down to load all contents on the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Sleep to allow content to load\n",
    "\n",
    "        # Find all articles\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, 'div.result-row.pub-row')\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # Extract title and URL\n",
    "                title_element = article.find_element(By.CSS_SELECTOR, 'h6 > a')\n",
    "                title = title_element.text.strip()\n",
    "                url = title_element.get_attribute('href').strip()\n",
    "\n",
    "                # Extract date\n",
    "                date = article.find_element(By.CSS_SELECTOR, 'p').text.strip()\n",
    "\n",
    "                # Extract description\n",
    "                description_element = article.find_element(By.CSS_SELECTOR, 'p:nth-of-type(2) > span')\n",
    "                description = description_element.text.strip()\n",
    "\n",
    "                # Write the data to the CSV file\n",
    "                writer.writerow([title, url, date, description])\n",
    "\n",
    "            except Exception as e:\n",
    "                pass \n",
    "\n",
    "# Close the driver after scraping\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c6820f-34b3-40da-bbf4-37a5d7015295",
   "metadata": {},
   "source": [
    "## WTO News/ Press"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cb62904-7f1b-4036-8f1b-b3c037e60536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to ChromeDriver\n",
    "s = Service(r'C:\\Users\\Victoria\\OneDrive\\文档\\Dropbox\\Machine Learning Python\\Project\\chromedriver.exe')\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# Base URL setup with a placeholder for a variable part, e.g., year\n",
    "base_url = 'https://www.wto.org/english/news_e/news{:02d}_e/news{:02d}_e.htm'\n",
    "\n",
    "# Range of years or other identifier to scrape\n",
    "starts = range(00,24,1)  \n",
    "# Prepare CSV file to store results\n",
    "with open('wto_text.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'URL', 'Date', 'Description'])  # Write header\n",
    "\n",
    "    for start in starts:\n",
    "        # Construct URL for each page by replacing the placeholder with the current 'start' value\n",
    "        url = base_url.format(start, start)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Allow some time for the page to load\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Scroll down to load all contents on the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Sleep to allow content to load\n",
    "\n",
    "        # Find all articles\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, 'tr')  # Assuming each 'tr' contains a news article\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # Extract date\n",
    "                date = article.find_element(By.CSS_SELECTOR, 'td:first-child').text.strip()\n",
    "\n",
    "                # Extract title\n",
    "                title = article.find_element(By.CSS_SELECTOR, 'h3.mt0.pt0').text.strip()\n",
    "\n",
    "                # Extract description\n",
    "                description = article.find_element(By.CSS_SELECTOR, 'p').text.strip()\n",
    "\n",
    "                # Extract URL\n",
    "                url_element = article.find_element(By.CSS_SELECTOR, 'a.paracolourtext')\n",
    "                url = url_element.get_attribute('href').strip()\n",
    "\n",
    "                # Write the data to the CSV file\n",
    "                writer.writerow([title, url, date, description])\n",
    "\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "# Close the driver after scraping\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "091bd824-ffee-43d9-9c0d-e9556c6c149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to ChromeDriver\n",
    "s = Service(r'C:\\Users\\Victoria\\OneDrive\\文档\\Dropbox\\Machine Learning Python\\Project\\chromedriver.exe')\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# Base URL setup with a placeholder for a variable part, e.g., year\n",
    "base_url = 'https://www.wto.org/english/news_e/pres{:02d}_e/pres{:02d}_e.htm'\n",
    "\n",
    "# Range of years or other identifier to scrape\n",
    "starts = range(6,24)  \n",
    "# Prepare CSV file to store results\n",
    "with open('wto_text2.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'URL', 'Date', 'Description'])  # Write header\n",
    "\n",
    "    for start in starts:\n",
    "        # Construct URL for each page by replacing the placeholder with the current 'start' value\n",
    "        url = base_url.format(start, start)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Allow some time for the page to load\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Scroll down to load all contents on the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Sleep to allow content to load\n",
    "\n",
    "        # Find all articles\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, 'tr')  # Assuming each 'tr' contains a news article\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # Extract date\n",
    "                date = article.find_element(By.CSS_SELECTOR, 'td:first-child').text.strip()\n",
    "\n",
    "                # Extract title\n",
    "                title = article.find_element(By.CSS_SELECTOR, 'h3.mt0.pt0').text.strip()\n",
    "\n",
    "                # Extract description\n",
    "                description = article.find_element(By.CSS_SELECTOR, 'p').text.strip()\n",
    "\n",
    "                # Extract URL\n",
    "                url_element = article.find_element(By.CSS_SELECTOR, 'a.paracolourtext')\n",
    "                url = url_element.get_attribute('href').strip()\n",
    "\n",
    "                # Write the data to the CSV file\n",
    "                writer.writerow([title, url, date, description])\n",
    "\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "# Close the driver after scraping\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e14badf-4d42-43f1-a4bd-5302bfdcc390",
   "metadata": {},
   "source": [
    " ## United Nation Global Economy News & Department of Economic and Social Affairs \n",
    "Economic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6cb12e3e-3364-44ec-a519-837b252ff6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to ChromeDriver\n",
    "s = Service(r'C:\\Users\\Victoria\\OneDrive\\文档\\Dropbox\\Machine Learning Python\\Project\\chromedriver.exe')\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# Base URL setup with a placeholder for a variable part, e.g., year\n",
    "base_url = 'https://news.un.org/en/tags/global-economy?page={}'\n",
    "\n",
    "# Range of years or other identifier to scrape\n",
    "starts = range(0,6)  \n",
    "# Prepare CSV file to store results\n",
    "with open('un_text.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'URL', 'Date', 'Description'])  # Write header\n",
    "\n",
    "    for start in starts:\n",
    "        # Construct URL for each page by replacing the placeholder with the current 'start' value\n",
    "        url = base_url.format(start, start)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Allow some time for the page to load\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Scroll down to load all contents on the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Sleep to allow content to load\n",
    "\n",
    "        # Find all articles (Assuming each article is within a 'div.col-lg-7')\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, 'div.col-lg-7')\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # Extract title and URL\n",
    "                title_element = article.find_element(By.CSS_SELECTOR, 'h2.node__title a')\n",
    "                title = title_element.text.strip()\n",
    "                url = title_element.get_attribute('href').strip()\n",
    "\n",
    "                # Extract date\n",
    "                date_element = article.find_element(By.CSS_SELECTOR, 'time.datetime')\n",
    "                date = date_element.text.strip()\n",
    "\n",
    "                # Extract description\n",
    "                description_element = article.find_element(By.CSS_SELECTOR, 'div.node__content p')\n",
    "                description = description_element.text.strip()\n",
    "\n",
    "                # Write the data to the CSV file\n",
    "                writer.writerow([title, url, date, description])\n",
    "\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "# Close the driver after scraping\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee653609-cfc8-4aec-9ba9-ef1b6e544f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to ChromeDriver\n",
    "s = Service(r'C:\\Users\\Victoria\\OneDrive\\文档\\Dropbox\\Machine Learning Python\\Project\\chromedriver.exe')\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# Base URL setup with a placeholder for a variable part, e.g., year\n",
    "base_url = 'https://www.un.org/development/desa/dpad/category/news-events/page/{}/'\n",
    "\n",
    "# Range of years or other identifier to scrape\n",
    "starts = range(0,72)  \n",
    "# Prepare CSV file to store results\n",
    "with open('un_text2.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'URL', 'Date', 'Description'])  # Write header\n",
    "\n",
    "    for start in starts:\n",
    "        # Construct URL for each page by replacing the placeholder with the current 'start' value\n",
    "        url = base_url.format(start, start)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Allow some time for the page to load\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Scroll down to load all contents on the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Sleep to allow content to load\n",
    "\n",
    "        # Find all articles (Assuming each article is within a 'div.col-lg-7')\n",
    "            # Find all articles within the specified container\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, 'div.col-sm-4')\n",
    "        for article in articles:\n",
    "                try:\n",
    "            # Extract title and URL\n",
    "                    title_element = article.find_element(By.CSS_SELECTOR, 'div.archive-item-title h3 a')\n",
    "                    title = title_element.text.strip()\n",
    "                    url = title_element.get_attribute('href').strip()\n",
    "\n",
    "            # Extract date\n",
    "                    date = article.find_element(By.CSS_SELECTOR, 'div.archive-item-date p').text.strip()\n",
    "\n",
    "            # Extract description\n",
    "                    description = article.find_element(By.CSS_SELECTOR, 'div.archive-item-excerpt').text.strip()\n",
    "\n",
    "\n",
    "                # Write the data to the CSV file\n",
    "                    writer.writerow([title, url, date, description])\n",
    "\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "# Close the driver after scraping\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "02f97a8d-687c-4a2b-80b8-b810f57dc2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of filenames to combine\n",
    "filenames = ['un_text2.csv','un_text.csv','wto_text2.csv','wto_text.csv','imf_text9.csv','imf_text8.csv','imf_text7.csv','imf_text6.csv',\n",
    "             'imf_text5.csv','imf_text4.csv','imf_text3.csv', 'imf_text2.csv', 'imf_text.csv']\n",
    "\n",
    "# Combine the CSV files and remove duplicates\n",
    "df_combined = pd.concat((pd.read_csv(filename) for filename in filenames), ignore_index=True).drop_duplicates()\n",
    "\n",
    "# Save the combined and cleaned data to a new CSV file\n",
    "df_combined.to_csv('text_combine.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf8d6f-0825-4875-ae6d-f97cb038dd54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
